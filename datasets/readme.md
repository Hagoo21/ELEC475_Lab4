# COCO-CLIP Dataset

This directory contains a subset of the COCO 2014 dataset prepared for CLIP-style image-text training.

## ğŸ“Š Dataset Summary

### Images
- **Train**: 2,000 images from COCO 2014 train split
- **Val**: 2,000 images from COCO 2014 validation split
- **Total**: 4,000 images
- **Format**: JPEG, various resolutions (resized to 224Ã—224 during training)

### Captions
- **Per Image**: 5 captions (COCO standard)
- **Total Captions**: 10,000 (train) + 10,000 (val) = 20,000 captions
- **Format**: COCO JSON format with image metadata and annotations
- **Content**: Human-written descriptions of image content

### Text Embeddings
- **Model**: CLIP ViT-B/32 text encoder (`openai/clip-vit-base-patch32`)
- **Dimension**: 512-dimensional vectors
- **Preprocessing**: 
  - Tokenization with max_length=77
  - Truncation and padding to max_length
  - Encoded using pretrained CLIP text encoder
- **Purpose**: Pre-computed to avoid redundant encoding during training (saves time and GPU memory)
- **Format**: PyTorch tensors (`.pt` files)

## ğŸ“ Directory Structure

```
datasets/
â”œâ”€â”€ coco_subset/                    # Main dataset
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/                # 2,000 JPEG images
â”‚   â”‚   â””â”€â”€ captions.json          # Image metadata + 10,000 captions
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ images/                # 2,000 JPEG images
â”‚       â””â”€â”€ captions.json          # Image metadata + 10,000 captions
â”‚
â”œâ”€â”€ cache/                          # Pre-computed embeddings
â”‚   â”œâ”€â”€ train_text_embeds.pt      # 2,000 CLIP text embeddings (train)
â”‚   â””â”€â”€ val_text_embeds.pt        # 2,000 CLIP text embeddings (val)
â”‚
â”œâ”€â”€ coco_annotations_cache/         # Full COCO caption files (for reference)
â”‚   â”œâ”€â”€ captions_train2014.json   # Original COCO train captions
â”‚   â”œâ”€â”€ captions_val2014.json     # Original COCO val captions
â”‚   â””â”€â”€ annotations_trainval2014.zip
â”‚
â”œâ”€â”€ eda_results/                    # Exploratory data analysis outputs
â”‚   â””â”€â”€ (generated by eda_coco_dataset.py)
â”‚
â”œâ”€â”€ prepare_coco_clip_dataset.py   # Dataset preparation script
â”œâ”€â”€ eda_coco_dataset.py            # Exploratory data analysis script
â””â”€â”€ readme.md                      # This file
```

## ğŸ” Caption JSON Format

Each `captions.json` file contains:

```json
{
  "images": [
    {
      "id": 233935,
      "file_name": "COCO_train2014_000000233935.jpg",
      "width": 640,
      "height": 400
    },
    ...
  ],
  "annotations": [
    {
      "id": 0,
      "image_id": 233935,
      "caption": "an air plane flying thru the sky"
    },
    {
      "id": 1,
      "image_id": 233935,
      "caption": "A small airplane is flying through clear skies."
    },
    ...
  ]
}
```

- **images**: Array of image metadata (id, filename, dimensions)
- **annotations**: Array of captions (id, image_id, caption text)
- Each `image_id` in annotations links to an `id` in images
- Multiple captions can have the same `image_id` (5 per image)

## ğŸ’¾ Storage Requirements

| Component | Size | Description |
|-----------|------|-------------|
| Train images | ~200-400 MB | 2,000 JPEG files |
| Val images | ~200-400 MB | 2,000 JPEG files |
| Train embeddings | ~4 MB | 2,000 Ã— 512 float32 tensors |
| Val embeddings | ~4 MB | 2,000 Ã— 512 float32 tensors |
| Caption JSON | ~10 MB | Text annotations |
| **Total** | **~500 MB - 1 GB** | Complete dataset |

## ğŸš€ Usage

### Load Dataset in Python

```python
from datasets.prepare_coco_clip_dataset import COCOCLIPDataset
import torch
from torch.utils.data import DataLoader

# Load pre-computed embeddings
train_embeddings = torch.load('datasets/cache/train_text_embeds.pt')

# Create dataset
train_dataset = COCOCLIPDataset(
    images_dir='datasets/coco_subset/train/images',
    captions_path='datasets/coco_subset/train/captions.json',
    text_embeddings=train_embeddings
)

# Create dataloader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# Iterate
for images, text_embeds, image_ids in train_loader:
    # images: (B, 3, 224, 224) - normalized image tensors
    # text_embeds: (B, 512) - pre-computed CLIP text embeddings
    # image_ids: (B,) - COCO image IDs
    pass
```

### Run Exploratory Data Analysis

```bash
python datasets/eda_coco_dataset.py
```

This generates comprehensive statistics and visualizations in `datasets/eda_results/`:
- Image dimension distributions
- Caption length statistics
- Word frequency analysis
- Embedding properties
- Sample visualizations
- Train vs Val comparisons

## ğŸ“ Notes

- Images are automatically resized to 224Ã—224 with CLIP normalization during training
- Text embeddings use only the first caption per image (remaining 4 captions available for future use)
- Dataset is deterministically sampled with `random_seed=42` for reproducibility
- Caption annotations cached from official COCO website (reused across runs)

## ğŸ”„ Re-running Preparation

To re-download the dataset (e.g., different samples):

1. Delete existing data: `datasets/coco_subset/` and `datasets/cache/`
2. Run: `python datasets/prepare_coco_clip_dataset.py`
3. Keep `datasets/coco_annotations_cache/` to avoid re-downloading captions (~19MB)
