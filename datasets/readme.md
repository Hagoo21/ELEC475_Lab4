# COCO-CLIP Dataset

This directory contains a subset of the COCO 2014 dataset prepared for CLIP-style image-text training.

## ğŸ“Š Dataset Summary

### Dataset Split Sizes
This project uses a **subset of the COCO 2014 dataset**:
- **Train**: 16,000 images (randomly sampled from COCO 2014 train split)
- **Val**: 4,000 images (randomly sampled from COCO 2014 validation split)
- **Total**: 20,000 images
- **Sampling**: Deterministic sampling with random seed 42 for reproducibility

### Images
- **Format**: JPEG, various original resolutions
- **Original Size**: Mixed (ranging from small to large images in the COCO dataset)

### Captions
- **Per Image**: 5 captions (COCO standard)
- **Total Captions**: 80,000 (train) + 20,000 (val) = 100,000 captions
- **Format**: COCO JSON format with image metadata and annotations
- **Content**: Human-written descriptions of image content

### Image Preprocessing Pipeline
Applied during training/inference:
1. **Resize**: All images resized to 224 Ã— 224 pixels
2. **Convert to Tensor**: Scales pixel values to [0, 1]
3. **Normalize**: Using CLIP-specific statistics
   - **Mean**: `[0.48145466, 0.4578275, 0.40821073]` (RGB)
   - **Std**: `[0.26862954, 0.26130258, 0.27577711]` (RGB)

### Text Preprocessing Details
- **Tokenizer**: CLIPTokenizer from `openai/clip-vit-base-patch32`
- **Tokenization Method**: CLIP's byte-pair encoding (BPE) tokenizer
- **Max Length**: 77 tokens (CLIP's fixed context length)
- **Truncation Strategy**: `truncation=True` (sequences longer than 77 tokens are truncated)
- **Padding Strategy**: `padding='max_length'` (all sequences padded to exactly 77 tokens)
- **Return Format**: PyTorch tensors (`return_tensors='pt'`)

### Text Embeddings (Precomputed)
- **Model**: CLIP ViT-B/32 text encoder (`openai/clip-vit-base-patch32`)
- **Dimension**: 512-dimensional vectors
- **Encoding**: Uses the pretrained CLIP text encoder's pooler output
- **Purpose**: Pre-computed to avoid redundant encoding during training (saves time and GPU memory)
- **Format**: PyTorch tensors (`.pt` files)
- **Caching Strategy**: Only the first caption per image is encoded and cached for training efficiency

## ğŸ“ Directory Structure

```
datasets/
â”œâ”€â”€ coco_subset/                    # Main dataset
â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ images/                # Training JPEG images
â”‚   â”‚   â””â”€â”€ captions.json          # Image metadata + training captions
â”‚   â””â”€â”€ val/
â”‚       â”œâ”€â”€ images/                # Validation JPEG images
â”‚       â””â”€â”€ captions.json          # Image metadata + validation captions
â”‚
â”œâ”€â”€ cache/                          # Pre-computed embeddings
â”‚   â”œâ”€â”€ train_text_embeds.pt      # CLIP text embeddings (train)
â”‚   â””â”€â”€ val_text_embeds.pt        # CLIP text embeddings (val)
â”‚
â”œâ”€â”€ coco_annotations_cache/         # Full COCO caption files (for reference)
â”‚   â”œâ”€â”€ captions_train2014.json   # Original COCO train captions
â”‚   â”œâ”€â”€ captions_val2014.json     # Original COCO val captions
â”‚   â””â”€â”€ annotations_trainval2014.zip
â”‚
â”œâ”€â”€ eda_results/                    # Exploratory data analysis outputs
â”‚   â””â”€â”€ (generated by eda_coco_dataset.py)
â”‚
â”œâ”€â”€ prepare_coco_clip_dataset.py   # Dataset preparation script
â”œâ”€â”€ eda_coco_dataset.py            # Exploratory data analysis script
â””â”€â”€ readme.md                      # This file
```

## ğŸ” Caption JSON Format

Each `captions.json` file contains:

```json
{
  "images": [
    {
      "id": 233935,
      "file_name": "COCO_train2014_000000233935.jpg",
      "width": 640,
      "height": 400
    },
    ...
  ],
  "annotations": [
    {
      "id": 0,
      "image_id": 233935,
      "caption": "an air plane flying thru the sky"
    },
    {
      "id": 1,
      "image_id": 233935,
      "caption": "A small airplane is flying through clear skies."
    },
    ...
  ]
}
```

- **images**: Array of image metadata (id, filename, dimensions)
- **annotations**: Array of captions (id, image_id, caption text)
- Each `image_id` in annotations links to an `id` in images
- Multiple captions can have the same `image_id` (5 per image)

## ğŸš€ Usage

### Load Dataset in Python

```python
from datasets.prepare_coco_clip_dataset import COCOCLIPDataset
import torch
from torch.utils.data import DataLoader

# Load pre-computed embeddings
train_embeddings = torch.load('datasets/cache/train_text_embeds.pt')

# Create dataset
train_dataset = COCOCLIPDataset(
    images_dir='datasets/coco_subset/train/images',
    captions_path='datasets/coco_subset/train/captions.json',
    text_embeddings=train_embeddings
)

# Create dataloader
train_loader = DataLoader(
    train_dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4
)

# Iterate
for images, text_embeds, image_ids in train_loader:
    # images: (B, 3, 224, 224) - normalized image tensors
    # text_embeds: (B, 512) - pre-computed CLIP text embeddings
    # image_ids: (B,) - COCO image IDs
    pass
```

### Run Exploratory Data Analysis

```bash
python datasets/eda_coco_dataset.py
```

This generates comprehensive statistics and visualizations in `datasets/eda_results/`:
- Image dimension distributions
- Caption length statistics
- Word frequency analysis
- Embedding properties
- Sample visualizations
- Train vs Val comparisons

## ğŸ“ Notes

- **Image Preprocessing**: Images are automatically resized to 224Ã—224 with CLIP normalization during training
- **Text Processing**: Text embeddings use only the first caption per image (remaining 4 captions available for future use or data augmentation)
- **Reproducibility**: Dataset is deterministically sampled with `random_seed=42` for reproducibility
- **Caching**: Caption annotations cached from official COCO website (reused across runs to avoid re-downloading)
- **Dataset Size**: Using a subset allows for faster experimentation while maintaining dataset diversity

## ğŸ”„ Re-running Preparation

To re-download the dataset (e.g., different samples):

1. Delete existing data: `datasets/coco_subset/` and `datasets/cache/`
2. Run: `python datasets/prepare_coco_clip_dataset.py`
3. Keep `datasets/coco_annotations_cache/` to avoid re-downloading captions (~19MB)
